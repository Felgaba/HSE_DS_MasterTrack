# -*- coding: utf-8 -*-
"""sem1_my_first_neural_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h3b8Bh6BuaDJ4wSSwA2Hr7dZahy4FWXZ

### Deadline 23.01.2022

# Первая нейросетка

Ну что ж, пришло время построить свою первую нейронную сеть. У нас будет искуственная задачка и мы посмотрим на её примере, как работают различные алгоритмы машинного обучения.
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

df = pd.read_csv('/content/drive/MyDrive/Programming/DL/homework (1)/hw_1/data.csv',index_col=0)

df.head()

"""Как и в любой задаче машинного обучения для начала надо визуализировать данные. 
Не зря же нашу нейронную сеть мы так долго учим?

"""

plt.figure(figsize=(10, 10))
sns.scatterplot(x="x1", y="x2", hue="y", data = df)

"""После просмотра данных встает несколько вопросов:
 * 1) Можем ли мы построить идеальную модель ? (условная метрика точность)
 * 2) Что ещё мы хотим знать о выборке?
 * 3) Какие алгоритмы машинного обучения мы можем эффективно здесь использовать?

После описательного анализа приступим к построению моделей. Для начала посмотрим, что нам даст Логистическая регрессия.
"""

from sklearn.model_selection import train_test_split 
from sklearn.metrics import accuracy_score

# разобьем выборку на трэйн/тест
X_train, X_test, y_train, y_test = train_test_split(df[['x1','x2']], df['y'], test_size = 0.33)

from sklearn.linear_model import LogisticRegression


#########################################
# Ваш код для Логистической регрессии
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
predict_logit = logreg.predict(X_test)
predict_log_proba = logreg.predict_proba(X_test)

#########################################


print(f'Точность нашей модели {accuracy_score(predict_logit, y_test)}')

# Заодно завизуализируем полученные результаты
plt.figure(figsize=(10, 10))
plt.scatter(X_test['x1'], X_test['x2'], c = predict_log_proba[:, 0]) #predict_log_proba
plt.show()

"""Вообще машинное обучение не только про алгоритмы (нейронные сети не исключение),
 а скорее про обработку данных. Что нам надо добавить, чтобы наш алгоритм отработал без проблем?

&emsp; *В начале показалось, что точки расположены внутри эллипса, но, посмотрев внимтельнее, можно обнаружить, что `range` значений одинаковый, а иллизия создана масштабом осей.*  
&emsp; *Таким образом, точки лежат на окружности радиуса $10$ и недостающий признак - это радиус точки. Удобство расчёта заключается в центре окружности в т. $(0;0)$.*
"""

# Повторяем наш код, только с новыми фичами - смотрим на магию
df_new = df.copy()

#########################################
# ваш код для фичей и новой модели

df_new['new_feature_1'] = np.sqrt( np.square(df_new['x1']) + np.square(df_new['x2']) )

X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(df_new[['x1', 'x2', 'new_feature_1']],
                                                                    df_new['y'], test_size = 0.33)

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
predict_logit = logreg.predict(X_test)
predict_log_proba = logreg.predict_proba(X_test)

#########################################

print(f'Точность нашей модели {accuracy_score(predict_logit,y_test_new)}')

plt.figure(figsize=(10, 10))
plt.scatter(X_test_new['x1'], X_test_new['x2'], c = predict_log_proba[:, 1])
plt.show()

"""&emsp; *Судя по картинке мы явно избавились от градиента, но теперь распределние выглядит хаотично: $50/50$, так сказать. Так дело не пойдёт.*

Переходим к следующиму классу алгоритмов - деревья решений. Какая особенность есть у деревьев? Какой параметр надо зафикисировать, чтобы эта особенность не испортила обучение?

&emsp; *Возможная особенность: глубина дерева. Если не ограничить глубину дерева, обучение будет идти, пока все элементы не будут разбиты на отдельные листья.* **Очень велик риск переобучения**.   
&emsp; *По ссылке на описание [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) можно найти очень параметров, среди которых, очевидно, зафиксируем `max_depth`, также можно `random_state` (в каждом узле дерева feautures выбираются случайно*) и `class_weight` (сбалансированность выборок в узлах - с учётом частоты появлений).*  
&emsp; *Если построить дефолтную модель, качество будет 79%, картинка вероятностей будет бинарная (чёткое деление на класс - лучше интерпретируется), если добавить фиксированные параметры, качетсво вырастет до 82% и картинка получит выраженный градиент (менее интерпретируема, но даёт лучшее качество).*  
*хотя у нас всего $3$ параметра, добавление параметра не будет лишним.  

**Вопрос:** *Всё-таки какой итог лучше? (Если правильно понимаю результат, выбрал бы бинарное разделение)*
"""

predict_tree_proba

from sklearn.tree import DecisionTreeClassifier

#########################################
# Ваш код
dtc = DecisionTreeClassifier(random_state=0, class_weight = 'balanced', max_depth = 10)
dtc.fit(X_train, y_train)
predict_tree = dtc.predict(X_test)
print(f'Точность нашей модели {accuracy_score(predict_tree, y_test)}')

predict_tree_proba = dtc.predict_proba(X_test)

#########################################

plt.figure(figsize=(10,10))
plt.scatter(X_test['x1'], X_test['x2'], c = predict_tree_proba[:, 0])
plt.show()

"""Поможет ли в данном случае ансамблирование? Когда в целом ансамбль дает хорошие результаты?

&emsp; [*Ансамблирование применяют когда:*](https://builtin.com/machine-learning/ensemble-model)  
* *Высокая дисперсия: Модель очень чувствительна к имеющимся для обучения параметрам;*
* *Low accuracy Низкая точность (Accuracy): Одна модель может быть недостаточной, чтобы fit весь датасет;*
* *Шум и смещение (bias): Модель базируется на слишком маолм числе параметров при прогнозировании (**наш случай**).*  

&emsp; *Среди приведённых случаев, когда примпняется ансамблирование, есть один пункт, который отвечает нашему случаю, поэтому стоит ожидать улучшения результатов от его применения.*

*Вообще говоря, Ансамблирование всегда (почти?) улучшает качество модели, но его [недостатки связаны](https://blogs.perficient.com/2019/11/07/predictive-model-ensembles-pros-and-cons/#:~:text=Cons%20of%20Model%20Ensembles&text=That%20is%2C%20ensembles%20cannot%20help,be%20more%20difficult%20to%20interpret.) с:*  
* *Невозможностью объяснить разность между sample и population;*  
* *Трудность интерпретации результатов: на выходе имеем некое среднее из моделей;*  
* *Вычислительная сложность и время исполнения.*
"""

from sklearn.ensemble import RandomForestClassifier

#########################################
# Ваш код
rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
rf.fit(X_train, y_train)
predict_forest = rf.predict(X_test)
print(f'Точность нашей модели {accuracy_score(predict_forest, y_test)}')

predict_forest_proba = rf.predict_proba(X_test)

#########################################

plt.figure(figsize=(10, 10))
plt.scatter(X_test['x1'], X_test['x2'], c = predict_forest_proba[:, 0])
plt.show()

"""ну и наконец мы добрались до нейронок - пора собрать нашу нейроночку под эту задачу.

Соберем простенькую нейронку - нам хватит 2х слоев по 5 нейронов в каждом. Смотрим на предыдущую тетрадку и копипастим все, что мы там видим! (можно для убыстрения процесса добавить callback с ранней остановкой)
"""

# Подгружаем tensorflow 
import tensorflow as tf
keras = tf.keras

print(tf.__version__)
print(keras.__version__) 

from tensorflow.keras.models import Sequential   # Последовательность слоёв
import tensorflow.keras.layers as L              # Разные слои
import tensorflow.keras.optimizers as opt        # Оптимизаторы

from tensorflow.keras.metrics import Accuracy

def get_new_model( ):
    acc = Accuracy()

    ###########################################################
    # Ваш код! 
    model = Sequential(name = 'DushBag')  # модели можно дать имя!
    
    # Добавляем в нашу модель первый слой из 5 нейронов
    model.add(L.Dense(5, input_dim = X_train_new.shape[1], kernel_initializer='random_normal'))

    # Добавляем функцию активации на первый слой 
    model.add(L.Activation('sigmoid'))

    # Добавляем ещё один слой из 5 нейронов
    model.add(L.Dense(5, kernel_initializer='random_normal'))
    model.add(L.Activation('sigmoid'))

    # На выходе мы должны получить вероятности того, что объект относится к разным классам 
    # Сделать такое преобразование позволяет softmax как функция активации
    # На выход будет идти 2 вероятности по числу классов
    model.add(L.Dense(2, activation='softmax', kernel_initializer = 'random_normal'))
    
    ###########################################################
    
    # В качестве оптимизации будем использовать Adam
    # Это такой специальный градиентный спуск, обсудим его в следущий раз
    optimizer = opt.Adam(lr=1e-3)

    # Собираем модель
    model.compile(loss = 'categorical_crossentropy', 
                  metrics=['acc'], 
                  optimizer=optimizer)
    
    return model

X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(df_new[['x1', 'x2', 'new_feature_1']], df_new['y'], test_size = 0.33)

print(X_train_new.shape)
print(X_test_new.shape)

from tensorflow.keras.utils import to_categorical
#to_categorical(df_new['y'])

model = get_new_model( )

model.summary()

mod_fit = model.fit(X_train_new, to_categorical(y_train_new), validation_split=0.2, epochs=20, verbose=1, batch_size=128)

#########################################
# Ваш код

predict_neural = model.predict(X_test_new)
predict_classes = tf.argmax(model.predict(X_test_new), axis=1)

print(f'Точность нашей модели {accuracy_score(predict_classes, y_test_new)}')
#########################################

plt.figure(figsize=(10, 10))
plt.scatter(X_test_new['x1'], X_test_new['x2'], c = predict_neural[:, 0])
plt.show()

"""На этом наше приключение сегодня закончилось. Вопросы максимально желательны :)

**Основной вопрос в том, какая картинка вероятностей (бинарная или непрерывная) лучше?**  
На последнем результате мы видим чёткий градиент по всем направлениям из центра окружности, что согласуется с исходным распределением данных.
"""